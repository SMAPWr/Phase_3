{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Detector - PL - Features extraction for Conv. & LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [this notebook](https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/classifier/final_classifier.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from klepto.archives import dir_archive\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "import fasttext\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'lstm'\n",
    "dim = 6*20 if MODEL == 'conv' else 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poleval 2019 data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes pre:\n",
    "    0 - non-harmful\n",
    "    1 - cyberbullying\n",
    "    2 - hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xc5 in position 26: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-503c7f2bdef7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hsd/Poleval2019/perfect_data.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mtweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xc5 in position 26: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('hsd/Poleval2019/perfect_data.pkl'):\n",
    "    with open('hsd/Poleval2019/train_texts.txt', 'r') as f:\n",
    "        tweets = f.readlines()\n",
    "    with open('hsd/Poleval2019/test_texts.txt', 'r') as f:\n",
    "        tweets.extend(f.readlines())\n",
    "    \n",
    "    with open('hsd/Poleval2019/train_labels.txt', 'r') as f:\n",
    "        labels = f.readlines()\n",
    "    with open('hsd/Poleval2019/test_labels.txt', 'r') as f:\n",
    "        labels.extend(f.readlines())\n",
    "    \n",
    "    with open('hsd/Poleval2019/perfect_data.pkl', 'w') as f:\n",
    "        def chcl(c):\n",
    "            return 0 if c=='0\\r\\n' else 1\n",
    "        labels = list(map(chcl, labels))\n",
    "        pickle.dump((tweets, labels), f)\n",
    "else:\n",
    "    with open('hsd/Poleval2019/perfect_data.pkl', 'rb') as f:\n",
    "        tweets, labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes post:\n",
    "    0 - no hate\n",
    "    1 - hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(tweets[:5], labels[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, '', parsed_text)\n",
    "    return parsed_text\n",
    "\n",
    "def pos(text):\n",
    "    import morfeusz2\n",
    "    morf = morfeusz2.Morfeusz()\n",
    "\n",
    "    analysis = morf.analyse(line)\n",
    "    \n",
    "    return [interp[2] for i, j, interp in analysis]\n",
    "    \n",
    "\n",
    "def pad_words(words, length):\n",
    "    if len(words) >= length:\n",
    "        return words[:length]\n",
    "    else:\n",
    "        additional = length - len(words)\n",
    "        return words + ['PUSTY']*additional\n",
    "\n",
    "def adjust_words(words, length):\n",
    "    # different from pad: output tokens may contain more than 1 words\n",
    "    if len(words) >= length:\n",
    "        q, r = divmod(len(words), length)\n",
    "        return [' '.join(words[i * q + min(i, r):(i + 1) * q + min(i + 1, r)]) for i in xrange(length)]\n",
    "    else:\n",
    "        additional = length - len(words)\n",
    "        return words + ['EMPTY']*additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if no morfeusz2 installed then save preprocessed tweets and load pos strings from outer source\n",
    "sentences = [preprocess(t) for t in tweets]\n",
    "with open('hsd/Poleval2019/preprocessed.pkl', 'w') as f:\n",
    "    pickle.dump(sentences, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median sentences length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_sentences_length(data):\n",
    "    all_lengths, wt_lengths, pos_lengths = [], [], []\n",
    "    for d in tqdm(data):\n",
    "        sentence = preprocess(d)\n",
    "        all_lengths.append(len(sentence.split(' ')))\n",
    "        wt_lengths.append(len(sentence.split(' ')))\n",
    "        \n",
    "        # only if morfeusz2 is installed\n",
    "        '''all_lengths.append(len(pos(preprocess(d))))\n",
    "        pos_lengths.append(len(pos(preprocess(d))))'''\n",
    "    # otherwise load pos strings from outer source\n",
    "    with open('hsd/Poleval2019/pos_sentences.pkl', 'r') as f:\n",
    "        pos_sentences = pickle.load(f)\n",
    "    for ps in tqdm(pos_sentences):\n",
    "        all_lengths.append(len(ps.split(' ')))\n",
    "        pos_lengths.append(len(ps.split(' ')))\n",
    "    \n",
    "    return int(np.median(all_lengths)), int(np.median(wt_lengths)), int(np.median(pos_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_length, opt_wt_length, opt_pos_length = median_sentences_length(tweets)\n",
    "\n",
    "print('Optimal all length: {}'.format(opt_length))\n",
    "print('Optimal sentence length: {}'.format(opt_wt_length))\n",
    "print('Optimal pos sentence length: {}'.format(opt_pos_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised fastText wordtokens training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('hsd/Poleval2019/fasttext.ft'):\n",
    "    with open('hsd/Poleval2019/fasttext.ft', 'a') as f:\n",
    "        for t, l in list(zip(tweets, labels)):\n",
    "            f.write('__label__{} {}\\n'.format(l, preprocess(t)))\n",
    "\n",
    "# load fasttext model or train & save if none\n",
    "if os.path.exists('hsd/Poleval2019/fasttext_{}.bin'.format(MODEL)):\n",
    "    ft_model = fasttext.load_model('hsd/Poleval2019/fasttext_{}.bin'.format(MODEL))\n",
    "else:\n",
    "    ft_model = fasttext.train_supervised('hsd/Poleval2019/fasttext.ft',\n",
    "                                         lr=0.5, epoch=50, wordNgrams=3, dim=dim)\n",
    "    ft_model.save_model('hsd/Poleval2019/fasttext_{}.bin'.format(MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordtoken features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordtoken_fts(data, length, adjust=False):\n",
    "    \n",
    "    sentences_words = []\n",
    "    t = tqdm(data)\n",
    "    t.set_postfix_str('Wordtokens features extraction: tokenization.')\n",
    "    for d in t:\n",
    "        sentence = preprocess(d)\n",
    "        sentences_words.append(sentence.split(' '))\n",
    "    \n",
    "    if adjust:\n",
    "        sentences_words = [adjust_words(sw, length) for sw in sentences_words]\n",
    "    else:\n",
    "        sentences_words = [pad_words(sw, length) for sw in sentences_words]\n",
    "    \n",
    "    ft_matrices = []\n",
    "    t = tqdm(sentences_words)\n",
    "    t.set_postfix_str('Wordtokens features extraction: vectorization.')\n",
    "    for sw in t:\n",
    "        ft_matrix = []\n",
    "        for w in sw:\n",
    "            ft_matrix.append(ft_model[w])\n",
    "        ft_matrices.append(ft_matrix)\n",
    "    \n",
    "    return ft_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtoken_features = get_wordtoken_fts(tweets, opt_wt_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtoken_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised fastText wordtokens training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('hsd/Poleval2019/fasttext_pos.ft'):\n",
    "    # only if morfeusz2 is installed\n",
    "    '''with open('hsd/Poleval2019/fasttext_pos.ft', 'a') as f:\n",
    "        for t, l in list(zip(tweets, labels)):\n",
    "            f.write('__label__{} {}\\n'.format(l, pos(t)))'''\n",
    "    # otherwise load pos strings from outer source\n",
    "    with open('hsd/Poleval2019/pos_sentences.pkl', 'r') as f:\n",
    "        pos_sentences = pickle.load(f)\n",
    "    with open('hsd/Poleval2019/fasttext_pos.ft', 'a') as f:\n",
    "        for ps, l in list(zip(pos_sentences, labels)):\n",
    "            f.write('__label__{} {}\\n'.format(l, ps))\n",
    "        \n",
    "\n",
    "# load fasttext pos model or train & save if none\n",
    "if os.path.exists('hsd/Poleval2019/fasttext_pos_{}.bin'.format(MODEL)):\n",
    "    ft_pos_model = fasttext.load_model('hsd/Poleval2019/fasttext_pos_{}.bin'.format(MODEL))\n",
    "else:\n",
    "    ft_pos_model = fasttext.train_supervised('hsd/Poleval2019/fasttext_pos.ft',\n",
    "                                             lr=0.5, epoch=50, wordNgrams=3, dim=dim)\n",
    "    ft_pos_model.save_model('hsd/Poleval2019/fasttext_pos_{}.bin'.format(MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech (PoS) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_fts(data, length, adjust=False, batch_data=None):\n",
    "    \n",
    "    # only if morfeusz2 is installed\n",
    "    '''pos_sentences = [pos(sentence) for sentence in tqdm(sentences)]'''\n",
    "    # otherwise load pos strings from outer source\n",
    "    if adjust and batch_data:  # for test data\n",
    "        pos_sentences = batch_data\n",
    "    else:\n",
    "        with open('hsd/Poleval2019/pos_sentences.pkl', 'r') as f:\n",
    "            pos_sentences = pickle.load(f)\n",
    "    \n",
    "    \n",
    "    pos_tags = []\n",
    "    for ps in pos_sentences:\n",
    "        pos_tags.append(ps.split(' '))\n",
    "    \n",
    "    if adjust:\n",
    "        pos_tags = [adjust_words(pt, length) for pt in pos_tags]\n",
    "    else:\n",
    "        pos_tags = [pad_words(pt, length) for pt in pos_tags]\n",
    "    \n",
    "    ft_matrices = []\n",
    "    t = tqdm(pos_tags)\n",
    "    t.set_postfix_str('PoS features extraction: tokenization.')\n",
    "    for pt in t:\n",
    "        ft_matrix = []\n",
    "        for t in pt:\n",
    "            ft_matrix.append(ft_pos_model[t])\n",
    "        ft_matrices.append(ft_matrix)\n",
    "    \n",
    "    return ft_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_features = get_pos_fts(tweets, opt_pos_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(wordtoken_features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(pos_features).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now join them all up\n",
    "features = np.concatenate([wordtoken_features, pos_features],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save features & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = dir_archive('hsd/Poleval2019/X_y_{}'.format(MODEL), {'features': features, 'labels': labels,\n",
    "                                                                'wt_num': np.array(wordtoken_features).shape[1]}, serialized=True)\n",
    "archive.dump()\n",
    "del archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Detector - PL - Features extraction for PL test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHES = 1\n",
    "\n",
    "TOKENS_LENGTH = 13\n",
    "POS_LENGTH = 27\n",
    "\n",
    "if os.path.exists('tests/tweets_pl.csv'):\n",
    "    with open('tests/tweets_pl.csv', 'r') as f:\n",
    "        raw_tweets = [d[0] for d in tqdm(list(csv.reader(f))[1:])]\n",
    "    batch_len = len(raw_tweets)/BATCHES\n",
    "    \n",
    "    # for polish test tweets\n",
    "    # if no morfeusz2 installed then save preprocessed tweets and load pos strings from outer source\n",
    "    sentences = [preprocess(t) for t in raw_tweets]\n",
    "    with open('tests/preprocessed_pl.pkl', 'w') as f:\n",
    "        pickle.dump(sentences, f)\n",
    "    \n",
    "    if not os.path.exists('tests/pos_sentences_pl.pkl'):\n",
    "        raise Exception('Stay awhile! Use morfeusz2 and get pos features.')\n",
    "    \n",
    "    with open('tests/pos_sentences_pl.pkl', 'r') as f:\n",
    "        pos_sentences = pickle.load(f)\n",
    "    \n",
    "    q, r = divmod(len(raw_tweets), BATCHES)\n",
    "    tweets_batches = [raw_tweets[i * q + min(i, r):(i + 1) * q + min(i + 1, r)] for i in xrange(BATCHES)]\n",
    "    pos_batches = [pos_sentences[i * q + min(i, r):(i + 1) * q + min(i + 1, r)] for i in xrange(BATCHES)]\n",
    "    \n",
    "    for batch in range(BATCHES):\n",
    "        print('Batch {}/{}'.format(batch+1, BATCHES))\n",
    "        tweets_batch = tweets_batches[batch]\n",
    "        pos_batch = pos_batches[batch]\n",
    "    \n",
    "        wt_features = get_wordtoken_fts(tweets_batch, length=TOKENS_LENGTH)\n",
    "        p_features = get_pos_fts(tweets_batch, length=POS_LENGTH, batch_data=pos_batch)\n",
    "\n",
    "        all_features = np.concatenate([wt_features, p_features], axis=1)\n",
    "        print('Done! Extracted dimensions: {}'.format(all_features.shape))\n",
    "        \n",
    "        batch_str = str(batch) if batch >= 100 else '0'+str(batch) if batch >= 10 else '00'+str(batch)\n",
    "        archive = dir_archive('tests/pl_{}/X_{}'.format(MODEL, batch_str), {'features': all_features,\n",
    "                                                                            'wt_num': np.array(wt_features).shape[1]}, serialized=True)\n",
    "        archive.dump()\n",
    "        del archive\n",
    "    print('All done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
